import { Example, FeedbackConfig, Run, ScoreType, ValueType } from "../schemas.js";
import { RunTreeConfig } from "../run_trees.js";

//#region ../../node_modules/.pnpm/langsmith@0.5.3_@opentelemetry+api@1.9.0_openai@5.23.2_ws@8.19.0_bufferutil@4.1.0__zod@4.3.6_/node_modules/langsmith/dist/evaluation/evaluator.d.ts
/**
 * Represents the result of an evaluation.
 */
type EvaluationResult = {
  /**
   * The key associated with the evaluation result.
   */
  key: string;
  /**
   * The score of the evaluation result.
   */
  score?: ScoreType;
  /**
   * The value of the evaluation result.
   */
  value?: ValueType;
  /**
   * A comment associated with the evaluation result.
   */
  comment?: string;
  /**
   * A correction record associated with the evaluation result.
   */
  correction?: Record<string, unknown>;
  /**
   * Information about the evaluator.
   */
  evaluatorInfo?: Record<string, unknown>;
  /**
   * The source run ID of the evaluation result.
   * If set, a link to the source run will be available in the UI.
   */
  sourceRunId?: string;
  /**
   * The target run ID of the evaluation result.
   * If this is not set, the target run ID is assumed to be
   * the root of the trace.
   */
  targetRunId?: string;
  /**
   * The feedback config associated with the evaluation result.
   * If set, this will be used to define how a feedback key
   * should be interpreted.
   */
  feedbackConfig?: FeedbackConfig;
};
/**
 * Batch evaluation results, if your evaluator wishes
 * to return multiple scores.
 */
type EvaluationResults = {
  /**
   * The evaluation results.
   */
  results: Array<EvaluationResult>;
};
interface RunEvaluator {
  evaluateRun(run: Run, example?: Example, options?: Partial<RunTreeConfig>): Promise<EvaluationResult | EvaluationResults>;
}
//#endregion
export { EvaluationResult, EvaluationResults, RunEvaluator };
//# sourceMappingURL=evaluator.d.ts.map