{"version":3,"file":"token_buffer_memory.js","names":[],"sources":["../../../../src/agents/toolkits/conversational_retrieval/token_buffer_memory.ts"],"sourcesContent":["import { ChatOpenAI } from \"@langchain/openai\";\nimport {\n  InputValues,\n  MemoryVariables,\n  OutputValues,\n  getInputValue,\n  getOutputValue,\n} from \"@langchain/core/memory\";\nimport { getBufferString } from \"@langchain/core/messages\";\nimport {\n  BaseChatMemory,\n  BaseChatMemoryInput,\n} from \"../../../memory/chat_memory.js\";\nimport { _formatIntermediateSteps } from \"../../openai_functions/index.js\";\n\n/**\n * Type definition for the fields required to initialize an instance of\n * OpenAIAgentTokenBufferMemory.\n */\nexport type OpenAIAgentTokenBufferMemoryFields = BaseChatMemoryInput & {\n  llm: ChatOpenAI;\n  humanPrefix?: string;\n  aiPrefix?: string;\n  memoryKey?: string;\n  maxTokenLimit?: number;\n  returnMessages?: boolean;\n  outputKey?: string;\n  intermediateStepsKey?: string;\n};\n\n/**\n * Memory used to save agent output and intermediate steps.\n */\nexport class OpenAIAgentTokenBufferMemory extends BaseChatMemory {\n  humanPrefix = \"Human\";\n\n  aiPrefix = \"AI\";\n\n  llm: ChatOpenAI;\n\n  memoryKey = \"history\";\n\n  maxTokenLimit = 12000;\n\n  returnMessages = true;\n\n  outputKey = \"output\";\n\n  intermediateStepsKey = \"intermediateSteps\";\n\n  constructor(fields: OpenAIAgentTokenBufferMemoryFields) {\n    super(fields);\n    this.humanPrefix = fields.humanPrefix ?? this.humanPrefix;\n    this.aiPrefix = fields.aiPrefix ?? this.aiPrefix;\n    this.llm = fields.llm;\n    this.memoryKey = fields.memoryKey ?? this.memoryKey;\n    this.maxTokenLimit = fields.maxTokenLimit ?? this.maxTokenLimit;\n    this.returnMessages = fields.returnMessages ?? this.returnMessages;\n    this.outputKey = fields.outputKey ?? this.outputKey;\n    this.intermediateStepsKey =\n      fields.intermediateStepsKey ?? this.intermediateStepsKey;\n  }\n\n  get memoryKeys(): string[] {\n    return [this.memoryKey];\n  }\n\n  /**\n   * Retrieves the messages from the chat history.\n   * @returns Promise that resolves with the messages from the chat history.\n   */\n  async getMessages() {\n    return this.chatHistory.getMessages();\n  }\n\n  /**\n   * Loads memory variables from the input values.\n   * @param _values Input values.\n   * @returns Promise that resolves with the loaded memory variables.\n   */\n  async loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\n    const buffer = await this.getMessages();\n    if (this.returnMessages) {\n      return { [this.memoryKey]: buffer };\n    } else {\n      const bufferString = getBufferString(\n        buffer,\n        this.humanPrefix,\n        this.aiPrefix\n      );\n      return { [this.memoryKey]: bufferString };\n    }\n  }\n\n  /**\n   * Saves the context of the chat, including user input, AI output, and\n   * intermediate steps. Prunes the chat history if the total token count\n   * exceeds the maximum limit.\n   * @param inputValues Input values.\n   * @param outputValues Output values.\n   * @returns Promise that resolves when the context has been saved.\n   */\n  async saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void> {\n    const inputValue = getInputValue(inputValues, this.inputKey);\n    const outputValue = getOutputValue(outputValues, this.outputKey);\n    await this.chatHistory.addUserMessage(inputValue);\n    const intermediateStepMessages = _formatIntermediateSteps(\n      outputValues[this.intermediateStepsKey]\n    );\n    for (const message of intermediateStepMessages) {\n      await this.chatHistory.addMessage(message);\n    }\n    await this.chatHistory.addAIMessage(outputValue);\n    const currentMessages = await this.chatHistory.getMessages();\n    let tokenInfo = await this.llm.getNumTokensFromMessages(currentMessages);\n    if (tokenInfo.totalCount > this.maxTokenLimit) {\n      const prunedMemory = [];\n      while (tokenInfo.totalCount > this.maxTokenLimit) {\n        const retainedMessage = currentMessages.pop();\n        if (!retainedMessage) {\n          console.warn(\n            `Could not prune enough messages from chat history to stay under ${this.maxTokenLimit} tokens.`\n          );\n          break;\n        }\n        prunedMemory.push(retainedMessage);\n        tokenInfo = await this.llm.getNumTokensFromMessages(currentMessages);\n      }\n      await this.chatHistory.clear();\n      for (const message of prunedMemory) {\n        await this.chatHistory.addMessage(message);\n      }\n    }\n  }\n}\n"],"mappings":";;;;;;;;;AAiCA,IAAa,+BAAb,cAAkD,eAAe;CAC/D,cAAc;CAEd,WAAW;CAEX;CAEA,YAAY;CAEZ,gBAAgB;CAEhB,iBAAiB;CAEjB,YAAY;CAEZ,uBAAuB;CAEvB,YAAY,QAA4C;AACtD,QAAM,OAAO;AACb,OAAK,cAAc,OAAO,eAAe,KAAK;AAC9C,OAAK,WAAW,OAAO,YAAY,KAAK;AACxC,OAAK,MAAM,OAAO;AAClB,OAAK,YAAY,OAAO,aAAa,KAAK;AAC1C,OAAK,gBAAgB,OAAO,iBAAiB,KAAK;AAClD,OAAK,iBAAiB,OAAO,kBAAkB,KAAK;AACpD,OAAK,YAAY,OAAO,aAAa,KAAK;AAC1C,OAAK,uBACH,OAAO,wBAAwB,KAAK;;CAGxC,IAAI,aAAuB;AACzB,SAAO,CAAC,KAAK,UAAU;;;;;;CAOzB,MAAM,cAAc;AAClB,SAAO,KAAK,YAAY,aAAa;;;;;;;CAQvC,MAAM,oBAAoB,SAAgD;EACxE,MAAM,SAAS,MAAM,KAAK,aAAa;AACvC,MAAI,KAAK,eACP,QAAO,GAAG,KAAK,YAAY,QAAQ;OAC9B;GACL,MAAM,eAAe,gBACnB,QACA,KAAK,aACL,KAAK,SACN;AACD,UAAO,GAAG,KAAK,YAAY,cAAc;;;;;;;;;;;CAY7C,MAAM,YACJ,aACA,cACe;EACf,MAAM,aAAa,cAAc,aAAa,KAAK,SAAS;EAC5D,MAAM,cAAc,eAAe,cAAc,KAAK,UAAU;AAChE,QAAM,KAAK,YAAY,eAAe,WAAW;EACjD,MAAM,2BAA2B,yBAC/B,aAAa,KAAK,sBACnB;AACD,OAAK,MAAM,WAAW,yBACpB,OAAM,KAAK,YAAY,WAAW,QAAQ;AAE5C,QAAM,KAAK,YAAY,aAAa,YAAY;EAChD,MAAM,kBAAkB,MAAM,KAAK,YAAY,aAAa;EAC5D,IAAI,YAAY,MAAM,KAAK,IAAI,yBAAyB,gBAAgB;AACxE,MAAI,UAAU,aAAa,KAAK,eAAe;GAC7C,MAAM,eAAe,EAAE;AACvB,UAAO,UAAU,aAAa,KAAK,eAAe;IAChD,MAAM,kBAAkB,gBAAgB,KAAK;AAC7C,QAAI,CAAC,iBAAiB;AACpB,aAAQ,KACN,mEAAmE,KAAK,cAAc,UACvF;AACD;;AAEF,iBAAa,KAAK,gBAAgB;AAClC,gBAAY,MAAM,KAAK,IAAI,yBAAyB,gBAAgB;;AAEtE,SAAM,KAAK,YAAY,OAAO;AAC9B,QAAK,MAAM,WAAW,aACpB,OAAM,KAAK,YAAY,WAAW,QAAQ"}