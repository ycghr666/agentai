{"version":3,"file":"deepinfra.js","names":[],"sources":["../../src/chat_models/deepinfra.ts"],"sourcesContent":["import {\n  BaseChatModel,\n  type BaseChatModelParams,\n  BindToolsInput,\n  type BaseChatModelCallOptions,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  AIMessage,\n  type BaseMessage,\n  type ToolMessage,\n  isAIMessage,\n  type UsageMetadata,\n  ChatMessage,\n  type AIMessageChunk,\n} from \"@langchain/core/messages\";\nimport {\n  convertLangChainToolCallToOpenAI,\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport { type ChatResult, type ChatGeneration } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { convertToOpenAITool } from \"@langchain/core/utils/function_calling\";\nimport { BaseLanguageModelInput } from \"@langchain/core/language_models/base\";\n\nexport const DEFAULT_MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\";\n\nexport type DeepInfraMessageRole = \"system\" | \"assistant\" | \"user\" | \"tool\";\n\nexport const API_BASE_URL =\n  \"https://api.deepinfra.com/v1/openai/chat/completions\";\n\nexport const ENV_VARIABLE_API_KEY = \"DEEPINFRA_API_TOKEN\";\n\ntype DeepInfraFinishReason = \"stop\" | \"length\" | \"tool_calls\" | \"null\" | null;\n\ninterface DeepInfraToolCall {\n  id: string;\n  type: \"function\";\n  function: {\n    name: string;\n    arguments: string;\n  };\n}\n\ninterface DeepInfraMessage {\n  role: DeepInfraMessageRole;\n  content: string;\n  tool_calls?: DeepInfraToolCall[];\n}\n\ninterface ChatCompletionRequest {\n  model: string;\n  messages?: DeepInfraMessage[];\n  stream?: boolean;\n  max_tokens?: number | null;\n  temperature?: number | null;\n  tools?: BindToolsInput[];\n  stop?: string[];\n}\n\ninterface BaseResponse {\n  code?: string;\n  message?: string;\n}\n\ninterface ChoiceMessage {\n  role: string;\n  content: string;\n  tool_calls?: DeepInfraToolCall[];\n}\n\ninterface ResponseChoice {\n  index: number;\n  finish_reason: DeepInfraFinishReason;\n  delta: ChoiceMessage;\n  message: ChoiceMessage;\n}\n\ninterface ChatCompletionResponse extends BaseResponse {\n  choices: ResponseChoice[];\n  usage: {\n    completion_tokens: number;\n    prompt_tokens: number;\n    total_tokens: number;\n  };\n  output: {\n    text: string;\n    finish_reason: DeepInfraFinishReason;\n  };\n}\n\nexport interface DeepInfraCallOptions extends BaseChatModelCallOptions {\n  stop?: string[];\n  tools?: BindToolsInput[];\n}\n\nexport interface ChatDeepInfraParams {\n  model: string;\n  apiKey?: string;\n  temperature?: number;\n  maxTokens?: number;\n}\n\nfunction messageToRole(message: BaseMessage): DeepInfraMessageRole {\n  const type = message._getType();\n  switch (type) {\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"system\":\n      return \"system\";\n    case \"tool\":\n      return \"tool\";\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\n\nfunction convertMessagesToDeepInfraParams(\n  messages: BaseMessage[]\n): DeepInfraMessage[] {\n  return messages.map((message): DeepInfraMessage => {\n    if (typeof message.content !== \"string\") {\n      throw new Error(\"Non string message content not supported\");\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const completionParam: Record<string, any> = {\n      role: messageToRole(message),\n      content: message.content,\n    };\n    if (message.name != null) {\n      completionParam.name = message.name;\n    }\n    if (isAIMessage(message) && !!message.tool_calls?.length) {\n      completionParam.tool_calls = message.tool_calls.map(\n        convertLangChainToolCallToOpenAI\n      );\n      completionParam.content = \"\";\n    } else {\n      if (message.additional_kwargs.tool_calls != null) {\n        completionParam.tool_calls = message.additional_kwargs.tool_calls;\n      }\n      if ((message as ToolMessage).tool_call_id != null) {\n        completionParam.tool_call_id = (message as ToolMessage).tool_call_id;\n      }\n    }\n    return completionParam as DeepInfraMessage;\n  });\n}\n\nfunction deepInfraResponseToChatMessage(\n  message: ChoiceMessage,\n  usageMetadata?: UsageMetadata\n): BaseMessage {\n  switch (message.role) {\n    case \"assistant\": {\n      const toolCalls = [];\n      const invalidToolCalls = [];\n      for (const rawToolCall of message.tool_calls ?? []) {\n        try {\n          toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));\n          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        } catch (e: any) {\n          invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n        }\n      }\n      return new AIMessage({\n        content: message.content || \"\",\n        additional_kwargs: { tool_calls: message.tool_calls ?? [] },\n        tool_calls: toolCalls,\n        invalid_tool_calls: invalidToolCalls,\n        usage_metadata: usageMetadata,\n      });\n    }\n    default:\n      return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n  }\n}\n\nexport class ChatDeepInfra\n  extends BaseChatModel<DeepInfraCallOptions>\n  implements ChatDeepInfraParams\n{\n  static lc_name() {\n    return \"ChatDeepInfra\";\n  }\n\n  get callKeys() {\n    return [\"stop\", \"signal\", \"options\", \"tools\"];\n  }\n\n  apiKey?: string;\n\n  model: string;\n\n  apiUrl: string;\n\n  maxTokens?: number;\n\n  temperature?: number;\n\n  constructor(fields: Partial<ChatDeepInfraParams> & BaseChatModelParams = {}) {\n    super(fields);\n\n    this.apiKey =\n      fields?.apiKey ?? getEnvironmentVariable(ENV_VARIABLE_API_KEY);\n    if (!this.apiKey) {\n      throw new Error(\n        \"API key is required, set `DEEPINFRA_API_TOKEN` environment variable or pass it as a parameter\"\n      );\n    }\n\n    this.apiUrl = API_BASE_URL;\n    this.model = fields.model ?? DEFAULT_MODEL;\n    this.temperature = fields.temperature ?? 0;\n    this.maxTokens = fields.maxTokens;\n  }\n\n  invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): Omit<ChatCompletionRequest, \"messages\"> {\n    if (options?.tool_choice) {\n      throw new Error(\n        \"Tool choice is not supported for ChatDeepInfra currently.\"\n      );\n    }\n    return {\n      model: this.model,\n      stream: false,\n      temperature: this.temperature,\n      max_tokens: this.maxTokens,\n      tools: options?.tools,\n      stop: options?.stop,\n    };\n  }\n\n  identifyingParams(): Omit<ChatCompletionRequest, \"messages\"> {\n    return this.invocationParams();\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options?: this[\"ParsedCallOptions\"]\n  ): Promise<ChatResult> {\n    const parameters = this.invocationParams(options);\n    const messagesMapped = convertMessagesToDeepInfraParams(messages);\n\n    const data: ChatCompletionResponse = await this.completionWithRetry(\n      { ...parameters, messages: messagesMapped },\n      false,\n      options?.signal\n    );\n\n    const {\n      prompt_tokens = 0,\n      completion_tokens = 0,\n      total_tokens = 0,\n    } = data.usage ?? {};\n\n    const usageMetadata: UsageMetadata = {\n      input_tokens: prompt_tokens,\n      output_tokens: completion_tokens,\n      total_tokens,\n    };\n    const generations: ChatGeneration[] = [];\n\n    for (const part of data?.choices ?? []) {\n      const text = part.message?.content ?? \"\";\n      const generation: ChatGeneration = {\n        text,\n        message: deepInfraResponseToChatMessage(part.message, usageMetadata),\n      };\n      if (part.finish_reason) {\n        generation.generationInfo = { finish_reason: part.finish_reason };\n      }\n      generations.push(generation);\n    }\n\n    return {\n      generations,\n      llmOutput: {\n        tokenUsage: {\n          promptTokens: prompt_tokens,\n          completionTokens: completion_tokens,\n          totalTokens: total_tokens,\n        },\n      },\n    };\n  }\n\n  async completionWithRetry(\n    request: ChatCompletionRequest,\n    stream: boolean,\n    signal?: AbortSignal\n  ): Promise<ChatCompletionResponse> {\n    const body = {\n      temperature: this.temperature,\n      max_tokens: this.maxTokens,\n      ...request,\n      model: this.model,\n    };\n\n    const makeCompletionRequest = async () => {\n      const response = await fetch(this.apiUrl, {\n        method: \"POST\",\n        headers: {\n          Authorization: `Bearer ${this.apiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify(body),\n        signal,\n      });\n\n      if (!stream) {\n        return response.json();\n      }\n    };\n\n    return this.caller.call(makeCompletionRequest);\n  }\n\n  override bindTools(\n    tools: BindToolsInput[],\n    kwargs?: Partial<DeepInfraCallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, DeepInfraCallOptions> {\n    return this.withConfig({\n      tools: tools.map((tool) => convertToOpenAITool(tool)),\n      ...kwargs,\n    } as DeepInfraCallOptions);\n  }\n\n  _llmType(): string {\n    return \"DeepInfra\";\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;AA0BA,MAAa,gBAAgB;AAI7B,MAAa,eACX;AAEF,MAAa,uBAAuB;AAwEpC,SAAS,cAAc,SAA4C;CACjE,MAAM,OAAO,QAAQ,UAAU;AAC/B,SAAQ,MAAR;EACE,KAAK,KACH,QAAO;EACT,KAAK,QACH,QAAO;EACT,KAAK,SACH,QAAO;EACT,KAAK,OACH,QAAO;EACT,QACE,OAAM,IAAI,MAAM,yBAAyB,OAAO;;;AAItD,SAAS,iCACP,UACoB;AACpB,QAAO,SAAS,KAAK,YAA8B;AACjD,MAAI,OAAO,QAAQ,YAAY,SAC7B,OAAM,IAAI,MAAM,2CAA2C;EAG7D,MAAM,kBAAuC;GAC3C,MAAM,cAAc,QAAQ;GAC5B,SAAS,QAAQ;GAClB;AACD,MAAI,QAAQ,QAAQ,KAClB,iBAAgB,OAAO,QAAQ;AAEjC,MAAI,YAAY,QAAQ,IAAI,CAAC,CAAC,QAAQ,YAAY,QAAQ;AACxD,mBAAgB,aAAa,QAAQ,WAAW,IAC9C,iCACD;AACD,mBAAgB,UAAU;SACrB;AACL,OAAI,QAAQ,kBAAkB,cAAc,KAC1C,iBAAgB,aAAa,QAAQ,kBAAkB;AAEzD,OAAK,QAAwB,gBAAgB,KAC3C,iBAAgB,eAAgB,QAAwB;;AAG5D,SAAO;GACP;;AAGJ,SAAS,+BACP,SACA,eACa;AACb,SAAQ,QAAQ,MAAhB;EACE,KAAK,aAAa;GAChB,MAAM,YAAY,EAAE;GACpB,MAAM,mBAAmB,EAAE;AAC3B,QAAK,MAAM,eAAe,QAAQ,cAAc,EAAE,CAChD,KAAI;AACF,cAAU,KAAK,cAAc,aAAa,EAAE,UAAU,MAAM,CAAC,CAAC;YAEvD,GAAQ;AACf,qBAAiB,KAAK,oBAAoB,aAAa,EAAE,QAAQ,CAAC;;AAGtE,UAAO,IAAI,UAAU;IACnB,SAAS,QAAQ,WAAW;IAC5B,mBAAmB,EAAE,YAAY,QAAQ,cAAc,EAAE,EAAE;IAC3D,YAAY;IACZ,oBAAoB;IACpB,gBAAgB;IACjB,CAAC;;EAEJ,QACE,QAAO,IAAI,YAAY,QAAQ,WAAW,IAAI,QAAQ,QAAQ,UAAU;;;AAI9E,IAAa,gBAAb,cACU,cAEV;CACE,OAAO,UAAU;AACf,SAAO;;CAGT,IAAI,WAAW;AACb,SAAO;GAAC;GAAQ;GAAU;GAAW;GAAQ;;CAG/C;CAEA;CAEA;CAEA;CAEA;CAEA,YAAY,SAA6D,EAAE,EAAE;AAC3E,QAAM,OAAO;AAEb,OAAK,SACH,QAAQ,UAAU,uBAAuB,qBAAqB;AAChE,MAAI,CAAC,KAAK,OACR,OAAM,IAAI,MACR,gGACD;AAGH,OAAK,SAAS;AACd,OAAK,QAAQ,OAAO,SAAS;AAC7B,OAAK,cAAc,OAAO,eAAe;AACzC,OAAK,YAAY,OAAO;;CAG1B,iBACE,SACyC;AACzC,MAAI,SAAS,YACX,OAAM,IAAI,MACR,4DACD;AAEH,SAAO;GACL,OAAO,KAAK;GACZ,QAAQ;GACR,aAAa,KAAK;GAClB,YAAY,KAAK;GACjB,OAAO,SAAS;GAChB,MAAM,SAAS;GAChB;;CAGH,oBAA6D;AAC3D,SAAO,KAAK,kBAAkB;;CAGhC,MAAM,UACJ,UACA,SACqB;EACrB,MAAM,aAAa,KAAK,iBAAiB,QAAQ;EACjD,MAAM,iBAAiB,iCAAiC,SAAS;EAEjE,MAAM,OAA+B,MAAM,KAAK,oBAC9C;GAAE,GAAG;GAAY,UAAU;GAAgB,EAC3C,OACA,SAAS,OACV;EAED,MAAM,EACJ,gBAAgB,GAChB,oBAAoB,GACpB,eAAe,MACb,KAAK,SAAS,EAAE;EAEpB,MAAM,gBAA+B;GACnC,cAAc;GACd,eAAe;GACf;GACD;EACD,MAAM,cAAgC,EAAE;AAExC,OAAK,MAAM,QAAQ,MAAM,WAAW,EAAE,EAAE;GAEtC,MAAM,aAA6B;IACjC,MAFW,KAAK,SAAS,WAAW;IAGpC,SAAS,+BAA+B,KAAK,SAAS,cAAc;IACrE;AACD,OAAI,KAAK,cACP,YAAW,iBAAiB,EAAE,eAAe,KAAK,eAAe;AAEnE,eAAY,KAAK,WAAW;;AAG9B,SAAO;GACL;GACA,WAAW,EACT,YAAY;IACV,cAAc;IACd,kBAAkB;IAClB,aAAa;IACd,EACF;GACF;;CAGH,MAAM,oBACJ,SACA,QACA,QACiC;EACjC,MAAM,OAAO;GACX,aAAa,KAAK;GAClB,YAAY,KAAK;GACjB,GAAG;GACH,OAAO,KAAK;GACb;EAED,MAAM,wBAAwB,YAAY;GACxC,MAAM,WAAW,MAAM,MAAM,KAAK,QAAQ;IACxC,QAAQ;IACR,SAAS;KACP,eAAe,UAAU,KAAK;KAC9B,gBAAgB;KACjB;IACD,MAAM,KAAK,UAAU,KAAK;IAC1B;IACD,CAAC;AAEF,OAAI,CAAC,OACH,QAAO,SAAS,MAAM;;AAI1B,SAAO,KAAK,OAAO,KAAK,sBAAsB;;CAGhD,AAAS,UACP,OACA,QACwE;AACxE,SAAO,KAAK,WAAW;GACrB,OAAO,MAAM,KAAK,SAAS,oBAAoB,KAAK,CAAC;GACrD,GAAG;GACJ,CAAyB;;CAG5B,WAAmB;AACjB,SAAO"}