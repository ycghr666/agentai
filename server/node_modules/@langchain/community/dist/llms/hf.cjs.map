{"version":3,"file":"hf.cjs","names":["LLM","GenerationChunk"],"sources":["../../src/llms/hf.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\n/**\n * Interface defining the parameters for configuring the Hugging Face\n * model for text generation.\n */\nexport interface HFInput {\n  /** Model to use */\n  model: string;\n\n  /** Custom inference endpoint URL to use */\n  endpointUrl?: string;\n\n  /** Sampling temperature to use */\n  temperature?: number;\n\n  /**\n   * Maximum number of tokens to generate in the completion.\n   */\n  maxTokens?: number;\n\n  /**\n   * The model will stop generating text when one of the strings in the list is generated.\n   */\n  stopSequences?: string[];\n\n  /** Total probability mass of tokens to consider at each step */\n  topP?: number;\n\n  /** Integer to define the top tokens considered within the sample operation to create new text. */\n  topK?: number;\n\n  /** Penalizes repeated tokens according to frequency */\n  frequencyPenalty?: number;\n\n  /** API key to use. */\n  apiKey?: string;\n\n  /**\n   * Credentials to use for the request. If this is a string, it will be passed straight on. If it's a boolean, true will be \"include\" and false will not send credentials at all.\n   */\n  includeCredentials?: string | boolean;\n}\n\n/**\n * Class implementing the Large Language Model (LLM) interface using the\n * Hugging Face Inference API for text generation.\n * @example\n * ```typescript\n * const model = new HuggingFaceInference({\n *   model: \"gpt2\",\n *   temperature: 0.7,\n *   maxTokens: 50,\n * });\n *\n * const res = await model.invoke(\n *   \"Question: What would be a good company name for a company that makes colorful socks?\\nAnswer:\"\n * );\n * console.log({ res });\n * ```\n */\nexport class HuggingFaceInference extends LLM implements HFInput {\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"HUGGINGFACEHUB_API_KEY\",\n    };\n  }\n\n  model = \"gpt2\";\n\n  temperature: number | undefined = undefined;\n\n  maxTokens: number | undefined = undefined;\n\n  stopSequences: string[] | undefined = undefined;\n\n  topP: number | undefined = undefined;\n\n  topK: number | undefined = undefined;\n\n  frequencyPenalty: number | undefined = undefined;\n\n  apiKey: string | undefined = undefined;\n\n  endpointUrl: string | undefined = undefined;\n\n  includeCredentials: string | boolean | undefined = undefined;\n\n  constructor(fields?: Partial<HFInput> & BaseLLMParams) {\n    super(fields ?? {});\n\n    this.model = fields?.model ?? this.model;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.stopSequences = fields?.stopSequences ?? this.stopSequences;\n    this.topP = fields?.topP ?? this.topP;\n    this.topK = fields?.topK ?? this.topK;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.apiKey =\n      fields?.apiKey ?? getEnvironmentVariable(\"HUGGINGFACEHUB_API_KEY\");\n    this.endpointUrl = fields?.endpointUrl;\n    this.includeCredentials = fields?.includeCredentials;\n\n    if (!this.apiKey) {\n      throw new Error(\n        `Please set an API key for HuggingFace Hub in the environment variable \"HUGGINGFACEHUB_API_KEY\" or in the apiKey field of the HuggingFaceInference constructor.`\n      );\n    }\n  }\n\n  _llmType() {\n    return \"hf\";\n  }\n\n  invocationParams(options?: this[\"ParsedCallOptions\"]) {\n    return {\n      model: this.model,\n      parameters: {\n        // make it behave similar to openai, returning only the generated text\n        return_full_text: false,\n        temperature: this.temperature,\n        max_new_tokens: this.maxTokens,\n        stop: options?.stop ?? this.stopSequences,\n        top_p: this.topP,\n        top_k: this.topK,\n        repetition_penalty: this.frequencyPenalty,\n      },\n    };\n  }\n\n  async *_streamResponseChunks(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    const hfi = await this._prepareHFInference();\n    const stream = await this.caller.call(async () =>\n      hfi.textGenerationStream({\n        ...this.invocationParams(options),\n        inputs: prompt,\n      })\n    );\n    for await (const chunk of stream) {\n      const token = chunk.token.text;\n      yield new GenerationChunk({ text: token, generationInfo: chunk });\n      await runManager?.handleLLMNewToken(token ?? \"\");\n\n      // stream is done\n      if (chunk.generated_text)\n        yield new GenerationChunk({\n          text: \"\",\n          generationInfo: { finished: true },\n        });\n    }\n  }\n\n  /** @ignore */\n  async _call(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"]\n  ): Promise<string> {\n    const hfi = await this._prepareHFInference();\n    const args = { ...this.invocationParams(options), inputs: prompt };\n    const res = await this.caller.callWithOptions(\n      { signal: options.signal },\n      hfi.textGeneration.bind(hfi),\n      args\n    );\n    return res.generated_text;\n  }\n\n  /** @ignore */\n  private async _prepareHFInference() {\n    const { HfInference } = await HuggingFaceInference.imports();\n    const hfi = new HfInference(this.apiKey, {\n      includeCredentials: this.includeCredentials,\n    });\n    return this.endpointUrl ? hfi.endpoint(this.endpointUrl) : hfi;\n  }\n\n  /** @ignore */\n  static async imports(): Promise<{\n    HfInference: typeof import(\"@huggingface/inference\").HfInference;\n  }> {\n    try {\n      const { HfInference } = await import(\"@huggingface/inference\");\n      return { HfInference };\n    } catch {\n      throw new Error(\n        \"Please install huggingface as a dependency with, e.g. `pnpm install @huggingface/inference`\"\n      );\n    }\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;AAgEA,IAAa,uBAAb,MAAa,6BAA6BA,yCAAuB;CAC/D,kBAAkB;CAElB,IAAI,aAAoD;AACtD,SAAO,EACL,QAAQ,0BACT;;CAGH,QAAQ;CAER,cAAkC;CAElC,YAAgC;CAEhC,gBAAsC;CAEtC,OAA2B;CAE3B,OAA2B;CAE3B,mBAAuC;CAEvC,SAA6B;CAE7B,cAAkC;CAElC,qBAAmD;CAEnD,YAAY,QAA2C;AACrD,QAAM,UAAU,EAAE,CAAC;AAEnB,OAAK,QAAQ,QAAQ,SAAS,KAAK;AACnC,OAAK,cAAc,QAAQ,eAAe,KAAK;AAC/C,OAAK,YAAY,QAAQ,aAAa,KAAK;AAC3C,OAAK,gBAAgB,QAAQ,iBAAiB,KAAK;AACnD,OAAK,OAAO,QAAQ,QAAQ,KAAK;AACjC,OAAK,OAAO,QAAQ,QAAQ,KAAK;AACjC,OAAK,mBAAmB,QAAQ,oBAAoB,KAAK;AACzD,OAAK,SACH,QAAQ,gEAAiC,yBAAyB;AACpE,OAAK,cAAc,QAAQ;AAC3B,OAAK,qBAAqB,QAAQ;AAElC,MAAI,CAAC,KAAK,OACR,OAAM,IAAI,MACR,iKACD;;CAIL,WAAW;AACT,SAAO;;CAGT,iBAAiB,SAAqC;AACpD,SAAO;GACL,OAAO,KAAK;GACZ,YAAY;IAEV,kBAAkB;IAClB,aAAa,KAAK;IAClB,gBAAgB,KAAK;IACrB,MAAM,SAAS,QAAQ,KAAK;IAC5B,OAAO,KAAK;IACZ,OAAO,KAAK;IACZ,oBAAoB,KAAK;IAC1B;GACF;;CAGH,OAAO,sBACL,QACA,SACA,YACiC;EACjC,MAAM,MAAM,MAAM,KAAK,qBAAqB;EAC5C,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YACpC,IAAI,qBAAqB;GACvB,GAAG,KAAK,iBAAiB,QAAQ;GACjC,QAAQ;GACT,CAAC,CACH;AACD,aAAW,MAAM,SAAS,QAAQ;GAChC,MAAM,QAAQ,MAAM,MAAM;AAC1B,SAAM,IAAIC,wCAAgB;IAAE,MAAM;IAAO,gBAAgB;IAAO,CAAC;AACjE,SAAM,YAAY,kBAAkB,SAAS,GAAG;AAGhD,OAAI,MAAM,eACR,OAAM,IAAIA,wCAAgB;IACxB,MAAM;IACN,gBAAgB,EAAE,UAAU,MAAM;IACnC,CAAC;;;;CAKR,MAAM,MACJ,QACA,SACiB;EACjB,MAAM,MAAM,MAAM,KAAK,qBAAqB;EAC5C,MAAM,OAAO;GAAE,GAAG,KAAK,iBAAiB,QAAQ;GAAE,QAAQ;GAAQ;AAMlE,UALY,MAAM,KAAK,OAAO,gBAC5B,EAAE,QAAQ,QAAQ,QAAQ,EAC1B,IAAI,eAAe,KAAK,IAAI,EAC5B,KACD,EACU;;;CAIb,MAAc,sBAAsB;EAClC,MAAM,EAAE,gBAAgB,MAAM,qBAAqB,SAAS;EAC5D,MAAM,MAAM,IAAI,YAAY,KAAK,QAAQ,EACvC,oBAAoB,KAAK,oBAC1B,CAAC;AACF,SAAO,KAAK,cAAc,IAAI,SAAS,KAAK,YAAY,GAAG;;;CAI7D,aAAa,UAEV;AACD,MAAI;GACF,MAAM,EAAE,gBAAgB,MAAM,OAAO;AACrC,UAAO,EAAE,aAAa;UAChB;AACN,SAAM,IAAI,MACR,8FACD"}