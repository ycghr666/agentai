{"version":3,"file":"llms.cjs","names":["BaseLanguageModel","CallbackManager","GenerationChunk","callbackHandlerPrefersStreaming","concat","RUN_KEY"],"sources":["../../src/language_models/llms.ts"],"sourcesContent":["import type { BasePromptValueInterface } from \"../prompt_values.js\";\nimport {\n  type LLMResult,\n  RUN_KEY,\n  type Generation,\n  GenerationChunk,\n} from \"../outputs.js\";\nimport {\n  type BaseCallbackConfig,\n  CallbackManager,\n  type CallbackManagerForLLMRun,\n  type Callbacks,\n} from \"../callbacks/manager.js\";\nimport {\n  BaseLanguageModel,\n  type BaseLanguageModelCallOptions,\n  type BaseLanguageModelInput,\n  type BaseLanguageModelParams,\n} from \"./base.js\";\nimport type { RunnableConfig } from \"../runnables/config.js\";\nimport type { BaseCache } from \"../caches/index.js\";\nimport { concat } from \"../utils/stream.js\";\nimport { callbackHandlerPrefersStreaming } from \"../callbacks/base.js\";\n\nexport type SerializedLLM = {\n  _model: string;\n  _type: string;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n} & Record<string, any>;\n\nexport interface BaseLLMParams extends BaseLanguageModelParams {}\n\nexport interface BaseLLMCallOptions extends BaseLanguageModelCallOptions {}\n\n/**\n * LLM Wrapper. Takes in a prompt (or prompts) and returns a string.\n */\nexport abstract class BaseLLM<\n  CallOptions extends BaseLLMCallOptions = BaseLLMCallOptions,\n> extends BaseLanguageModel<string, CallOptions> {\n  // Backwards compatibility since fields have been moved to RunnableConfig\n  declare ParsedCallOptions: Omit<\n    CallOptions,\n    Exclude<keyof RunnableConfig, \"signal\" | \"timeout\" | \"maxConcurrency\">\n  >;\n\n  // Only ever instantiated in main LangChain\n  lc_namespace = [\"langchain\", \"llms\", this._llmType()];\n\n  /**\n   * This method takes an input and options, and returns a string. It\n   * converts the input to a prompt value and generates a result based on\n   * the prompt.\n   * @param input Input for the LLM.\n   * @param options Options for the LLM call.\n   * @returns A string result based on the prompt.\n   */\n  async invoke(\n    input: BaseLanguageModelInput,\n    options?: Partial<CallOptions>\n  ): Promise<string> {\n    const promptValue = BaseLLM._convertInputToPromptValue(input);\n    const result = await this.generatePrompt(\n      [promptValue],\n      options,\n      options?.callbacks\n    );\n    return result.generations[0][0].text;\n  }\n\n  // eslint-disable-next-line require-yield\n  async *_streamResponseChunks(\n    _input: string,\n    _options: this[\"ParsedCallOptions\"],\n    _runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    throw new Error(\"Not implemented.\");\n  }\n\n  protected _separateRunnableConfigFromCallOptionsCompat(\n    options?: Partial<CallOptions>\n  ): [RunnableConfig, this[\"ParsedCallOptions\"]] {\n    // For backwards compat, keep `signal` in both runnableConfig and callOptions\n    const [runnableConfig, callOptions] =\n      super._separateRunnableConfigFromCallOptions(options);\n    (callOptions as this[\"ParsedCallOptions\"]).signal = runnableConfig.signal;\n    return [runnableConfig, callOptions as this[\"ParsedCallOptions\"]];\n  }\n\n  async *_streamIterator(\n    input: BaseLanguageModelInput,\n    options?: Partial<CallOptions>\n  ): AsyncGenerator<string> {\n    // Subclass check required to avoid double callbacks with default implementation\n    if (\n      this._streamResponseChunks === BaseLLM.prototype._streamResponseChunks\n    ) {\n      yield this.invoke(input, options);\n    } else {\n      const prompt = BaseLLM._convertInputToPromptValue(input);\n      const [runnableConfig, callOptions] =\n        this._separateRunnableConfigFromCallOptionsCompat(options);\n      const callbackManager_ = await CallbackManager.configure(\n        runnableConfig.callbacks,\n        this.callbacks,\n        runnableConfig.tags,\n        this.tags,\n        runnableConfig.metadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: callOptions,\n        invocation_params: this?.invocationParams(callOptions),\n        batch_size: 1,\n      };\n      const runManagers = await callbackManager_?.handleLLMStart(\n        this.toJSON(),\n        [prompt.toString()],\n        runnableConfig.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        runnableConfig.runName\n      );\n      let generation = new GenerationChunk({\n        text: \"\",\n      });\n      try {\n        for await (const chunk of this._streamResponseChunks(\n          prompt.toString(),\n          callOptions,\n          runManagers?.[0]\n        )) {\n          if (!generation) {\n            generation = chunk;\n          } else {\n            generation = generation.concat(chunk);\n          }\n          if (typeof chunk.text === \"string\") {\n            yield chunk.text;\n          }\n        }\n      } catch (err) {\n        await Promise.all(\n          (runManagers ?? []).map((runManager) =>\n            runManager?.handleLLMError(err)\n          )\n        );\n        throw err;\n      }\n      await Promise.all(\n        (runManagers ?? []).map((runManager) =>\n          runManager?.handleLLMEnd({\n            generations: [[generation]],\n          })\n        )\n      );\n    }\n  }\n\n  /**\n   * This method takes prompt values, options, and callbacks, and generates\n   * a result based on the prompts.\n   * @param promptValues Prompt values for the LLM.\n   * @param options Options for the LLM call.\n   * @param callbacks Callbacks for the LLM call.\n   * @returns An LLMResult based on the prompts.\n   */\n  async generatePrompt(\n    promptValues: BasePromptValueInterface[],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    const prompts: string[] = promptValues.map((promptValue) =>\n      promptValue.toString()\n    );\n    return this.generate(prompts, options, callbacks);\n  }\n\n  /**\n   * Run the LLM on the given prompts and input.\n   */\n  abstract _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult>;\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options?: this[\"ParsedCallOptions\"]): any {\n    return {};\n  }\n\n  _flattenLLMResult(llmResult: LLMResult): LLMResult[] {\n    const llmResults: LLMResult[] = [];\n\n    for (let i = 0; i < llmResult.generations.length; i += 1) {\n      const genList = llmResult.generations[i];\n\n      if (i === 0) {\n        llmResults.push({\n          generations: [genList],\n          llmOutput: llmResult.llmOutput,\n        });\n      } else {\n        const llmOutput = llmResult.llmOutput\n          ? { ...llmResult.llmOutput, tokenUsage: {} }\n          : undefined;\n\n        llmResults.push({\n          generations: [genList],\n          llmOutput,\n        });\n      }\n    }\n\n    return llmResults;\n  }\n\n  /** @ignore */\n  async _generateUncached(\n    prompts: string[],\n    parsedOptions: this[\"ParsedCallOptions\"],\n    handledOptions: BaseCallbackConfig,\n    startedRunManagers?: CallbackManagerForLLMRun[]\n  ): Promise<LLMResult> {\n    let runManagers: CallbackManagerForLLMRun[] | undefined;\n    if (\n      startedRunManagers !== undefined &&\n      startedRunManagers.length === prompts.length\n    ) {\n      runManagers = startedRunManagers;\n    } else {\n      const callbackManager_ = await CallbackManager.configure(\n        handledOptions.callbacks,\n        this.callbacks,\n        handledOptions.tags,\n        this.tags,\n        handledOptions.metadata,\n        this.metadata,\n        { verbose: this.verbose }\n      );\n      const extra = {\n        options: parsedOptions,\n        invocation_params: this?.invocationParams(parsedOptions),\n        batch_size: prompts.length,\n      };\n      runManagers = await callbackManager_?.handleLLMStart(\n        this.toJSON(),\n        prompts,\n        handledOptions.runId,\n        undefined,\n        extra,\n        undefined,\n        undefined,\n        handledOptions?.runName\n      );\n    }\n    // Even if stream is not explicitly called, check if model is implicitly\n    // called from streamEvents() or streamLog() to get all streamed events.\n    // Bail out if _streamResponseChunks not overridden\n    const hasStreamingHandler = !!runManagers?.[0].handlers.find(\n      callbackHandlerPrefersStreaming\n    );\n    let output: LLMResult;\n    if (\n      hasStreamingHandler &&\n      prompts.length === 1 &&\n      this._streamResponseChunks !== BaseLLM.prototype._streamResponseChunks\n    ) {\n      try {\n        const stream = await this._streamResponseChunks(\n          prompts[0],\n          parsedOptions,\n          runManagers?.[0]\n        );\n        let aggregated;\n        for await (const chunk of stream) {\n          if (aggregated === undefined) {\n            aggregated = chunk;\n          } else {\n            aggregated = concat(aggregated, chunk);\n          }\n        }\n        if (aggregated === undefined) {\n          throw new Error(\"Received empty response from chat model call.\");\n        }\n        output = { generations: [[aggregated]], llmOutput: {} };\n        await runManagers?.[0].handleLLMEnd(output);\n      } catch (e) {\n        await runManagers?.[0].handleLLMError(e);\n        throw e;\n      }\n    } else {\n      try {\n        output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n      } catch (err) {\n        await Promise.all(\n          (runManagers ?? []).map((runManager) =>\n            runManager?.handleLLMError(err)\n          )\n        );\n        throw err;\n      }\n\n      const flattenedOutputs: LLMResult[] = this._flattenLLMResult(output);\n      await Promise.all(\n        (runManagers ?? []).map((runManager, i) =>\n          runManager?.handleLLMEnd(flattenedOutputs[i])\n        )\n      );\n    }\n    const runIds = runManagers?.map((manager) => manager.runId) || undefined;\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runIds ? { runIds } : undefined,\n      configurable: true,\n    });\n    return output;\n  }\n\n  async _generateCached({\n    prompts,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions,\n    runId,\n  }: {\n    prompts: string[];\n    cache: BaseCache<Generation[]>;\n    llmStringKey: string;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    parsedOptions: any;\n    handledOptions: RunnableConfig;\n    runId?: string;\n  }): Promise<\n    LLMResult & {\n      missingPromptIndices: number[];\n      startedRunManagers?: CallbackManagerForLLMRun[];\n    }\n  > {\n    const callbackManager_ = await CallbackManager.configure(\n      handledOptions.callbacks,\n      this.callbacks,\n      handledOptions.tags,\n      this.tags,\n      handledOptions.metadata,\n      this.metadata,\n      { verbose: this.verbose }\n    );\n    const extra = {\n      options: parsedOptions,\n      invocation_params: this?.invocationParams(parsedOptions),\n      batch_size: prompts.length,\n    };\n    const runManagers = await callbackManager_?.handleLLMStart(\n      this.toJSON(),\n      prompts,\n      runId,\n      undefined,\n      extra,\n      undefined,\n      undefined,\n      handledOptions?.runName\n    );\n\n    // generate results\n    const missingPromptIndices: number[] = [];\n    const results = await Promise.allSettled(\n      prompts.map(async (prompt, index) => {\n        const result = await cache.lookup(prompt, llmStringKey);\n        if (result == null) {\n          missingPromptIndices.push(index);\n        }\n        return result;\n      })\n    );\n\n    // Map run managers to the results before filtering out null results\n    // Null results are just absent from the cache.\n    const cachedResults = results\n      .map((result, index) => ({ result, runManager: runManagers?.[index] }))\n      .filter(\n        ({ result }) =>\n          (result.status === \"fulfilled\" && result.value != null) ||\n          result.status === \"rejected\"\n      );\n\n    // Handle results and call run managers\n    const generations: Generation[][] = [];\n    await Promise.all(\n      cachedResults.map(async ({ result: promiseResult, runManager }, i) => {\n        if (promiseResult.status === \"fulfilled\") {\n          const result = promiseResult.value as Generation[];\n          generations[i] = result.map((result) => {\n            result.generationInfo = {\n              ...result.generationInfo,\n              tokenUsage: {},\n            };\n            return result;\n          });\n          if (result.length) {\n            await runManager?.handleLLMNewToken(result[0].text);\n          }\n          return runManager?.handleLLMEnd(\n            {\n              generations: [result],\n            },\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n        } else {\n          // status === \"rejected\"\n          await runManager?.handleLLMError(\n            promiseResult.reason,\n            undefined,\n            undefined,\n            undefined,\n            {\n              cached: true,\n            }\n          );\n          return Promise.reject(promiseResult.reason);\n        }\n      })\n    );\n\n    const output = {\n      generations,\n      missingPromptIndices,\n      startedRunManagers: runManagers,\n    };\n\n    // This defines RUN_KEY as a non-enumerable property on the output object\n    // so that it is not serialized when the output is stringified, and so that\n    // it isnt included when listing the keys of the output object.\n    Object.defineProperty(output, RUN_KEY, {\n      value: runManagers\n        ? { runIds: runManagers?.map((manager) => manager.runId) }\n        : undefined,\n      configurable: true,\n    });\n\n    return output;\n  }\n\n  /**\n   * Run the LLM on the given prompts and input, handling caching.\n   */\n  async generate(\n    prompts: string[],\n    options?: string[] | Partial<CallOptions>,\n    callbacks?: Callbacks\n  ): Promise<LLMResult> {\n    if (!Array.isArray(prompts)) {\n      throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n    }\n\n    let parsedOptions: Partial<CallOptions> | undefined;\n    if (Array.isArray(options)) {\n      parsedOptions = { stop: options } as Partial<CallOptions>;\n    } else {\n      parsedOptions = options;\n    }\n\n    const [runnableConfig, callOptions] =\n      this._separateRunnableConfigFromCallOptionsCompat(parsedOptions);\n    runnableConfig.callbacks = runnableConfig.callbacks ?? callbacks;\n\n    if (!this.cache) {\n      return this._generateUncached(prompts, callOptions, runnableConfig);\n    }\n\n    const { cache } = this;\n    const llmStringKey = this._getSerializedCacheKeyParametersForCall(\n      callOptions as CallOptions\n    );\n    const { generations, missingPromptIndices, startedRunManagers } =\n      await this._generateCached({\n        prompts,\n        cache,\n        llmStringKey,\n        parsedOptions: callOptions,\n        handledOptions: runnableConfig,\n        runId: runnableConfig.runId,\n      });\n\n    let llmOutput = {};\n    if (missingPromptIndices.length > 0) {\n      const results = await this._generateUncached(\n        missingPromptIndices.map((i) => prompts[i]),\n        callOptions,\n        runnableConfig,\n        startedRunManagers !== undefined\n          ? missingPromptIndices.map((i) => startedRunManagers?.[i])\n          : undefined\n      );\n      await Promise.all(\n        results.generations.map(async (generation, index) => {\n          const promptIndex = missingPromptIndices[index];\n          generations[promptIndex] = generation;\n          return cache.update(prompts[promptIndex], llmStringKey, generation);\n        })\n      );\n      llmOutput = results.llmOutput ?? {};\n    }\n\n    return { generations, llmOutput } as LLMResult;\n  }\n\n  /**\n   * Get the identifying parameters of the LLM.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  _identifyingParams(): Record<string, any> {\n    return {};\n  }\n\n  /**\n   * Return the string type key uniquely identifying this class of LLM.\n   */\n  abstract _llmType(): string;\n\n  _modelType(): string {\n    return \"base_llm\" as const;\n  }\n}\n\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nexport abstract class LLM<\n  CallOptions extends BaseLLMCallOptions = BaseLLMCallOptions,\n> extends BaseLLM<CallOptions> {\n  /**\n   * Run the LLM on the given prompt and input.\n   */\n  abstract _call(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<string>;\n\n  async _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const generations: Generation[][] = await Promise.all(\n      prompts.map((prompt, promptIndex) =>\n        this._call(prompt, { ...options, promptIndex }, runManager).then(\n          (text) => [{ text }]\n        )\n      )\n    );\n    return { generations };\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;AAqCA,IAAsB,UAAtB,MAAsB,gBAEZA,+CAAuC;CAQ/C,eAAe;EAAC;EAAa;EAAQ,KAAK,UAAU;EAAC;;;;;;;;;CAUrD,MAAM,OACJ,OACA,SACiB;EACjB,MAAM,cAAc,QAAQ,2BAA2B,MAAM;AAM7D,UALe,MAAM,KAAK,eACxB,CAAC,YAAY,EACb,SACA,SAAS,UACV,EACa,YAAY,GAAG,GAAG;;CAIlC,OAAO,sBACL,QACA,UACA,aACiC;AACjC,QAAM,IAAI,MAAM,mBAAmB;;CAGrC,AAAU,6CACR,SAC6C;EAE7C,MAAM,CAAC,gBAAgB,eACrB,MAAM,uCAAuC,QAAQ;AACvD,EAAC,YAA0C,SAAS,eAAe;AACnE,SAAO,CAAC,gBAAgB,YAAyC;;CAGnE,OAAO,gBACL,OACA,SACwB;AAExB,MACE,KAAK,0BAA0B,QAAQ,UAAU,sBAEjD,OAAM,KAAK,OAAO,OAAO,QAAQ;OAC5B;GACL,MAAM,SAAS,QAAQ,2BAA2B,MAAM;GACxD,MAAM,CAAC,gBAAgB,eACrB,KAAK,6CAA6C,QAAQ;GAC5D,MAAM,mBAAmB,MAAMC,0CAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,SAAS,CAC1B;GACD,MAAM,QAAQ;IACZ,SAAS;IACT,mBAAmB,MAAM,iBAAiB,YAAY;IACtD,YAAY;IACb;GACD,MAAM,cAAc,MAAM,kBAAkB,eAC1C,KAAK,QAAQ,EACb,CAAC,OAAO,UAAU,CAAC,EACnB,eAAe,OACf,QACA,OACA,QACA,QACA,eAAe,QAChB;GACD,IAAI,aAAa,IAAIC,gCAAgB,EACnC,MAAM,IACP,CAAC;AACF,OAAI;AACF,eAAW,MAAM,SAAS,KAAK,sBAC7B,OAAO,UAAU,EACjB,aACA,cAAc,GACf,EAAE;AACD,SAAI,CAAC,WACH,cAAa;SAEb,cAAa,WAAW,OAAO,MAAM;AAEvC,SAAI,OAAO,MAAM,SAAS,SACxB,OAAM,MAAM;;YAGT,KAAK;AACZ,UAAM,QAAQ,KACX,eAAe,EAAE,EAAE,KAAK,eACvB,YAAY,eAAe,IAAI,CAChC,CACF;AACD,UAAM;;AAER,SAAM,QAAQ,KACX,eAAe,EAAE,EAAE,KAAK,eACvB,YAAY,aAAa,EACvB,aAAa,CAAC,CAAC,WAAW,CAAC,EAC5B,CAAC,CACH,CACF;;;;;;;;;;;CAYL,MAAM,eACJ,cACA,SACA,WACoB;EACpB,MAAM,UAAoB,aAAa,KAAK,gBAC1C,YAAY,UAAU,CACvB;AACD,SAAO,KAAK,SAAS,SAAS,SAAS,UAAU;;;;;CAgBnD,iBAAiB,UAA2C;AAC1D,SAAO,EAAE;;CAGX,kBAAkB,WAAmC;EACnD,MAAM,aAA0B,EAAE;AAElC,OAAK,IAAI,IAAI,GAAG,IAAI,UAAU,YAAY,QAAQ,KAAK,GAAG;GACxD,MAAM,UAAU,UAAU,YAAY;AAEtC,OAAI,MAAM,EACR,YAAW,KAAK;IACd,aAAa,CAAC,QAAQ;IACtB,WAAW,UAAU;IACtB,CAAC;QACG;IACL,MAAM,YAAY,UAAU,YACxB;KAAE,GAAG,UAAU;KAAW,YAAY,EAAE;KAAE,GAC1C;AAEJ,eAAW,KAAK;KACd,aAAa,CAAC,QAAQ;KACtB;KACD,CAAC;;;AAIN,SAAO;;;CAIT,MAAM,kBACJ,SACA,eACA,gBACA,oBACoB;EACpB,IAAI;AACJ,MACE,uBAAuB,UACvB,mBAAmB,WAAW,QAAQ,OAEtC,eAAc;OACT;GACL,MAAM,mBAAmB,MAAMD,0CAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,SAAS,CAC1B;GACD,MAAM,QAAQ;IACZ,SAAS;IACT,mBAAmB,MAAM,iBAAiB,cAAc;IACxD,YAAY,QAAQ;IACrB;AACD,iBAAc,MAAM,kBAAkB,eACpC,KAAK,QAAQ,EACb,SACA,eAAe,OACf,QACA,OACA,QACA,QACA,gBAAgB,QACjB;;EAKH,MAAM,sBAAsB,CAAC,CAAC,cAAc,GAAG,SAAS,KACtDE,uDACD;EACD,IAAI;AACJ,MACE,uBACA,QAAQ,WAAW,KACnB,KAAK,0BAA0B,QAAQ,UAAU,sBAEjD,KAAI;GACF,MAAM,SAAS,MAAM,KAAK,sBACxB,QAAQ,IACR,eACA,cAAc,GACf;GACD,IAAI;AACJ,cAAW,MAAM,SAAS,OACxB,KAAI,eAAe,OACjB,cAAa;OAEb,cAAaC,4BAAO,YAAY,MAAM;AAG1C,OAAI,eAAe,OACjB,OAAM,IAAI,MAAM,gDAAgD;AAElE,YAAS;IAAE,aAAa,CAAC,CAAC,WAAW,CAAC;IAAE,WAAW,EAAE;IAAE;AACvD,SAAM,cAAc,GAAG,aAAa,OAAO;WACpC,GAAG;AACV,SAAM,cAAc,GAAG,eAAe,EAAE;AACxC,SAAM;;OAEH;AACL,OAAI;AACF,aAAS,MAAM,KAAK,UAAU,SAAS,eAAe,cAAc,GAAG;YAChE,KAAK;AACZ,UAAM,QAAQ,KACX,eAAe,EAAE,EAAE,KAAK,eACvB,YAAY,eAAe,IAAI,CAChC,CACF;AACD,UAAM;;GAGR,MAAM,mBAAgC,KAAK,kBAAkB,OAAO;AACpE,SAAM,QAAQ,KACX,eAAe,EAAE,EAAE,KAAK,YAAY,MACnC,YAAY,aAAa,iBAAiB,GAAG,CAC9C,CACF;;EAEH,MAAM,SAAS,aAAa,KAAK,YAAY,QAAQ,MAAM,IAAI;AAI/D,SAAO,eAAe,QAAQC,yBAAS;GACrC,OAAO,SAAS,EAAE,QAAQ,GAAG;GAC7B,cAAc;GACf,CAAC;AACF,SAAO;;CAGT,MAAM,gBAAgB,EACpB,SACA,OACA,cACA,eACA,gBACA,SAcA;EACA,MAAM,mBAAmB,MAAMJ,0CAAgB,UAC7C,eAAe,WACf,KAAK,WACL,eAAe,MACf,KAAK,MACL,eAAe,UACf,KAAK,UACL,EAAE,SAAS,KAAK,SAAS,CAC1B;EACD,MAAM,QAAQ;GACZ,SAAS;GACT,mBAAmB,MAAM,iBAAiB,cAAc;GACxD,YAAY,QAAQ;GACrB;EACD,MAAM,cAAc,MAAM,kBAAkB,eAC1C,KAAK,QAAQ,EACb,SACA,OACA,QACA,OACA,QACA,QACA,gBAAgB,QACjB;EAGD,MAAM,uBAAiC,EAAE;EAazC,MAAM,iBAZU,MAAM,QAAQ,WAC5B,QAAQ,IAAI,OAAO,QAAQ,UAAU;GACnC,MAAM,SAAS,MAAM,MAAM,OAAO,QAAQ,aAAa;AACvD,OAAI,UAAU,KACZ,sBAAqB,KAAK,MAAM;AAElC,UAAO;IACP,CACH,EAKE,KAAK,QAAQ,WAAW;GAAE;GAAQ,YAAY,cAAc;GAAQ,EAAE,CACtE,QACE,EAAE,aACA,OAAO,WAAW,eAAe,OAAO,SAAS,QAClD,OAAO,WAAW,WACrB;EAGH,MAAM,cAA8B,EAAE;AACtC,QAAM,QAAQ,IACZ,cAAc,IAAI,OAAO,EAAE,QAAQ,eAAe,cAAc,MAAM;AACpE,OAAI,cAAc,WAAW,aAAa;IACxC,MAAM,SAAS,cAAc;AAC7B,gBAAY,KAAK,OAAO,KAAK,WAAW;AACtC,YAAO,iBAAiB;MACtB,GAAG,OAAO;MACV,YAAY,EAAE;MACf;AACD,YAAO;MACP;AACF,QAAI,OAAO,OACT,OAAM,YAAY,kBAAkB,OAAO,GAAG,KAAK;AAErD,WAAO,YAAY,aACjB,EACE,aAAa,CAAC,OAAO,EACtB,EACD,QACA,QACA,QACA,EACE,QAAQ,MACT,CACF;UACI;AAEL,UAAM,YAAY,eAChB,cAAc,QACd,QACA,QACA,QACA,EACE,QAAQ,MACT,CACF;AACD,WAAO,QAAQ,OAAO,cAAc,OAAO;;IAE7C,CACH;EAED,MAAM,SAAS;GACb;GACA;GACA,oBAAoB;GACrB;AAKD,SAAO,eAAe,QAAQI,yBAAS;GACrC,OAAO,cACH,EAAE,QAAQ,aAAa,KAAK,YAAY,QAAQ,MAAM,EAAE,GACxD;GACJ,cAAc;GACf,CAAC;AAEF,SAAO;;;;;CAMT,MAAM,SACJ,SACA,SACA,WACoB;AACpB,MAAI,CAAC,MAAM,QAAQ,QAAQ,CACzB,OAAM,IAAI,MAAM,kDAAkD;EAGpE,IAAI;AACJ,MAAI,MAAM,QAAQ,QAAQ,CACxB,iBAAgB,EAAE,MAAM,SAAS;MAEjC,iBAAgB;EAGlB,MAAM,CAAC,gBAAgB,eACrB,KAAK,6CAA6C,cAAc;AAClE,iBAAe,YAAY,eAAe,aAAa;AAEvD,MAAI,CAAC,KAAK,MACR,QAAO,KAAK,kBAAkB,SAAS,aAAa,eAAe;EAGrE,MAAM,EAAE,UAAU;EAClB,MAAM,eAAe,KAAK,wCACxB,YACD;EACD,MAAM,EAAE,aAAa,sBAAsB,uBACzC,MAAM,KAAK,gBAAgB;GACzB;GACA;GACA;GACA,eAAe;GACf,gBAAgB;GAChB,OAAO,eAAe;GACvB,CAAC;EAEJ,IAAI,YAAY,EAAE;AAClB,MAAI,qBAAqB,SAAS,GAAG;GACnC,MAAM,UAAU,MAAM,KAAK,kBACzB,qBAAqB,KAAK,MAAM,QAAQ,GAAG,EAC3C,aACA,gBACA,uBAAuB,SACnB,qBAAqB,KAAK,MAAM,qBAAqB,GAAG,GACxD,OACL;AACD,SAAM,QAAQ,IACZ,QAAQ,YAAY,IAAI,OAAO,YAAY,UAAU;IACnD,MAAM,cAAc,qBAAqB;AACzC,gBAAY,eAAe;AAC3B,WAAO,MAAM,OAAO,QAAQ,cAAc,cAAc,WAAW;KACnE,CACH;AACD,eAAY,QAAQ,aAAa,EAAE;;AAGrC,SAAO;GAAE;GAAa;GAAW;;;;;CAOnC,qBAA0C;AACxC,SAAO,EAAE;;CAQX,aAAqB;AACnB,SAAO;;;;;;;;;;AAWX,IAAsB,MAAtB,cAEU,QAAqB;CAU7B,MAAM,UACJ,SACA,SACA,YACoB;AAQpB,SAAO,EAAE,aAP2B,MAAM,QAAQ,IAChD,QAAQ,KAAK,QAAQ,gBACnB,KAAK,MAAM,QAAQ;GAAE,GAAG;GAAS;GAAa,EAAE,WAAW,CAAC,MACzD,SAAS,CAAC,EAAE,MAAM,CAAC,CACrB,CACF,CACF,EACqB"}